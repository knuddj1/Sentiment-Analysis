{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special Topic - Machine Learning  - Semester Recap\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the semester, my project was to create a sentiment analysis engine to classify pieces of text into one of three different categories: positive, netural and negative. In this notebook I detail each step of the process that I went through, throughout the semester. \n",
    "\n",
    "<b>Below is a list of each sections covered:</b>\n",
    "- Hyperparameter fine tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining a training dataset\n",
    "--\n",
    "\n",
    "My first task was to obtain a dataset (or combination of datasets) that contained pieces of text with sufficent amounts data in each of the three categories. I looked at various datasets from kaggle but found that the number of neutral samples was severly lacking compared to positive and negative. I decided on using an amazon review dataset that can be found here http://jmcauley.ucsd.edu/data/amazon/.\n",
    "\n",
    "The dataset used on the website has the following description:\n",
    "<i>\"5-core (9.9gb) - subset of the data in which all users and items have at least 5 reviews (41.13 million reviews).\"</i>\n",
    "\n",
    "As the description states The dataset had over 9.9gb worth of data, however only a portion of the data was required for the model. The next section explains how I processed the dataset. The dataset is made up of a list of categories shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Books (8,898,041 reviews)\n",
    "- Electronics (1,689,188 reviews)\n",
    "- Movies and TV (1,697,533 reviews)\n",
    "- CDs and Vinyl (1,097,592 reviews)\n",
    "- Clothing, Shoes and Jewelry (278,677 reviews)\n",
    "- Home and Kitchen (551,682 reviews)\n",
    "- Kindle Store (982,619 reviews)\n",
    "- Sports and Outdoors (296,337 reviews)\n",
    "- Cell Phones and Accessories (194,439 reviews)\n",
    "- Health and Personal Care (346,355 reviews)\n",
    "- Toys and Games (167,597 reviews)\n",
    "- Video Games (231,780 reviews)\n",
    "- Tools and Home Improvement (134,476 reviews)\n",
    "- Beauty (198,502 reviews)\n",
    "- Apps for Android (752,937 reviews)\n",
    "- Office Products (53,258 reviews)\n",
    "- Pet Supplies (157,836 reviews)\n",
    "- Automotive (20,473 reviews)\n",
    "- Grocery and Gourmet Food (151,254 reviews)\n",
    "- Patio, Lawn and Garden (13,272 reviews)\n",
    "- Baby (160,792 reviews)\n",
    "- Digital Music (64,706 reviews)\n",
    "- Musical Instruments\t(10,261 reviews)\n",
    "- Amazon Instant Video (37,126 reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the dataset\n",
    "--\n",
    "\n",
    "Each sample within the dataset is a user submited review \n",
    "\n",
    "An example of a sample from the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    {\n",
    "      \"reviewerID\": \"A2SUAM1J3GNN3B\",\n",
    "      \"asin\": \"0000013714\",\n",
    "      \"reviewerName\": \"J. McDonald\",\n",
    "      \"helpful\": [2, 3],\n",
    "      \"reviewText\": \"I bought this for my husband who plays the piano. He is having a wonderful time playing these old hymns. The music is at times hard to read because we think the book was published for singing from more than playing from. Great purchase though!\",\n",
    "      \"overall\": 5.0,\n",
    "      \"summary\": \"Heavenly Highway Hymns\",\n",
    "      \"unixReviewTime\": 1252800000,\n",
    "      \"reviewTime\": \"09 13, 2009\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only parts of the samples I was interested in were <i>reviewText</i> and <i>overall</i>, the rest was discarded. The reviewText being the data and overall being the labels. Only reviews with an overall of '1.0', '3.0' and '5.0' were used as well, '2.0' and '4.0' being discarded. Reviews with an overall of '1.0' were given a label of '-1' representing negative, '3.0' were given a label of '0' representing netural and '5.0' were given a label of '1' representing positive.\n",
    "\n",
    "This new dataset resulted in a very unbalanced amount of samples between each category; The number of positive samples being greater than the number of neutral samples and the number of neutral being greater than negative. To solve this issue I took the total number of negative samples in each of the dataset's categories and only used that number of postive and neutral samples. This resulted in <i>789,327</i> positive, neutral and negative samples, creating a total dataset of <i>2,367,981</i> data points, reducing the dataset to a size of only 1.5gb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning and preparing the dataset\n",
    "--\n",
    "\n",
    "Machine learning models do not work well with raw text. So before the was data ready to be fed into a model I had to convert it to numerical form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Cleaning</b>\n",
    "\n",
    "First I cleaned each sample using a series of filters listed below from the standard python and genism libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert to lowercase\n",
    "- Strip away URLs\n",
    "- Remove tags\n",
    "- Remove non-alphabetic characters\n",
    "- Replace punctuation characters with spaces\n",
    "- Remove digits\n",
    "- Remove repeating whitespace characters (spaces, tabs, line breaks)\n",
    "- Remove stopwords using set of 339 stopwords from Stone, Denis, Kwantes (2010).\n",
    "- Remove words with length lesser than minsize = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Dictionaries</b>\n",
    "\n",
    "Next I mapped every unique word and the frequency of which that word occured in the cleaned dataset into a dictionary i.e {'good': 45000}. I then chose the top 30000 words to be included in the model and created two new dictionaries.\n",
    "\n",
    "The first of these dictionaries mapping words to integers:\n",
    "\n",
    "{'good': 52}\n",
    "\n",
    "The other of these dictionaries mapping integers to words:\n",
    "\n",
    "{52: 'good'}\n",
    "\n",
    "These dictionaries are in the range from 1-30001 because zero is reserved as a placeholder for words in the dataset that are not present in the top 30000 and to help with padding samples so they are the same length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Preparing inputs</b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample was placed into a list of size 500. Samples that were less than 500 words were padded with zeroes and samples with more than 500 words had the remainder discarded. Using the dictionary to map words to integers, each word was replaced with its correct integer representation. Words that were not present in this dictionary were replaced with a 0.\n",
    "\n",
    "Labels were one hot encoded into an array of lists of size 3, where each slot in the list is a 0 except one, which is a 1. For example a neutral label would be represented as [0,1,0]. This link explains what one encoding is and how to do it https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a test dataset\n",
    "--\n",
    "\n",
    "The accuracy a model achieves during training is not an accurate measurement of how well it is able to generalize on the problem at hand. It could just be memorizing the training set. Therefore a test dataset is required to get a more precise measurement of how general the model really is. David and I constructed a test dataset of 100 hand picked pieces of text from a range of different websites. The dataset is linked below: https://docs.google.com/spreadsheets/d/1_KPk66j24N7sTi_1h5cgchNFDChDGHuRWUO7ur5lJIE/edit#gid=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establishing a baseline\n",
    "--\n",
    "\n",
    "A baseline is useful to compare future models to see improvement/lack of improvement. Randoming guessing the label of a sample, one would expect a result of around 33.333333% on this problem (because of 3 labels). My first baseline model is listed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see it didn't achieve very good results on the test dataset, only a miserable xx%. It is OK if your baseline has a poor result. It may indicate a particular difficulty with the problem or it may mean that your algorithms have a lot of room for improvement. In the case with this model it is a plain vanilla neural network and doesnt have the size or capablities of future models that I experimented without throughout the semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing a grid search\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
